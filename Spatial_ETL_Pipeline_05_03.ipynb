{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrução de Banco de Dados Espaciais para Monitoramento de Desmatamento e Gestão Fundiária\n",
    "Implementando um pipeline de ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependências necessárias para o script\n",
    "\n",
    "%pip install pandas -q\n",
    "%pip install numpy -q\n",
    "%pip install geopandas -q\n",
    "%pip install psycopg2 -q\n",
    "%pip install sqlalchemy -q\n",
    "%pip install geoalchemy2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from shapely.validation import make_valid\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import psycopg2\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import shapely\n",
    "import fiona\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mapclassify -q\n",
    "%pip install seaborn -q\n",
    "%pip install lonboard -q\n",
    "%pip install pyarrow -q\n",
    "%pip install geopandas -q\n",
    "%pip install geoarrow-pyarrow -q\n",
    "%pip install geoarrow-pandas -q\n",
    "%pip install pyspark -q\n",
    "%pip install findspark -q\n",
    "%pip install duckdb -q\n",
    "%pip install folium -q\n",
    "%pip install matplotlib -q\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display\n",
    "from pyspark.sql.functions import round\n",
    "from IPython.display import display, HTML\n",
    "from lonboard import viz\n",
    "\n",
    "import seaborn as sns\n",
    "import pyspark.sql.functions as F\n",
    "import findspark\n",
    "import duckdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_files(source_dir, dest_dir):\n",
    "    \"\"\"\n",
    "    Extrai arquivos ZIP de um diretório de origem para um diretório de destino.\n",
    "    \n",
    "    :param source_dir: Caminho para o diretório que contém os arquivos ZIP.\n",
    "    :param dest_dir: Caminho para o diretório onde os arquivos serão extraídos.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando a extração de arquivos ZIP...\")\n",
    "    try:\n",
    "        # Verificar se o diretório de origem existe\n",
    "        if not os.path.exists(source_dir):\n",
    "            raise FileNotFoundError(f\"O diretório de origem não existe: {source_dir}\")\n",
    "\n",
    "        # Criar o diretório de destino, se necessário\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "        # Iterar sobre os arquivos no diretório de origem\n",
    "        for file_name in os.listdir(source_dir):\n",
    "            if file_name.endswith(\".zip\"):\n",
    "                zip_path = os.path.join(source_dir, file_name)\n",
    "                output_subdir = os.path.join(dest_dir, os.path.splitext(file_name)[0])  # Subdiretório com o nome do ZIP\n",
    "\n",
    "                try:\n",
    "                    print(f\"Extraindo: {zip_path} para {output_subdir}...\")\n",
    "                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(output_subdir)  # Extrair para o subdiretório correspondente\n",
    "                    print(f\"Extração concluída: {zip_path}\")\n",
    "                except zipfile.BadZipFile:\n",
    "                    print(f\"Erro: O arquivo {zip_path} não é um ZIP válido. Ignorando...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao extrair {zip_path}: {e}\")\n",
    "\n",
    "        print(\"Extração concluída para todos os arquivos ZIP.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante a extração: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretórios\n",
    "raw_dir = r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\"\n",
    "extracted_dir = os.path.join(raw_dir, \"extracted\")\n",
    "output_dir = os.path.join(raw_dir, \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretórios criados ou já existentes.\n",
      "Iniciando o pipeline...\n",
      "Iniciando a extração de arquivos ZIP...\n",
      "Iniciando a extração de arquivos ZIP...\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\adm_embargo_ibama_a.shp.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\adm_embargo_ibama_a.shp...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\adm_embargo_ibama_a.shp.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\amazonia.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\amazonia...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\amazonia.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\base_fundiaria_imaflora.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\base_fundiaria_imaflora...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\base_fundiaria_imaflora.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\biomas_5000.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\biomas_5000...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\biomas_5000.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\BR_Municipios_2023_IBGE.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\BR_Municipios_2023_IBGE...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\BR_Municipios_2023_IBGE.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\BR_UF_2023_IBGE.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\BR_UF_2023_IBGE...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\BR_UF_2023_IBGE.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\caatinga.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\caatinga...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\caatinga.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\cerrado_pantanal.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\cerrado_pantanal...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\cerrado_pantanal.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\deter-amz-16-01-2025-23_27_34.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\deter-amz-16-01-2025-23_27_34...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\deter-amz-16-01-2025-23_27_34.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\mataatlantica.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\mataatlantica...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\mataatlantica.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\pampa.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\pampa...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\pampa.zip\n",
      "Extraindo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\pa_br_deforestation_prodes_2002-2022_IMAFLORA.zip para C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\extracted\\pa_br_deforestation_prodes_2002-2022_IMAFLORA...\n",
      "Extração concluída: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\pa_br_deforestation_prodes_2002-2022_IMAFLORA.zip\n",
      "Extração concluída para todos os arquivos ZIP.\n",
      "Extração concluída com sucesso!\n"
     ]
    }
   ],
   "source": [
    "def extract():\n",
    "    \"\"\"\n",
    "    Função principal para executar o pipeline de extração de arquivos ZIP.\n",
    "    \"\"\"\n",
    "    # Criar diretórios de saída, se necessário\n",
    "    try:\n",
    "        os.makedirs(extracted_dir, exist_ok=True)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(\"Diretórios criados ou já existentes.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar diretórios: {e}\")\n",
    "        return\n",
    "\n",
    "    # Pipeline\n",
    "    print(\"Iniciando o pipeline...\")\n",
    "    try:\n",
    "        print(\"Iniciando a extração de arquivos ZIP...\")\n",
    "        extract_zip_files(raw_dir, extracted_dir)\n",
    "        print(\"Extração concluída com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro crítico no pipeline: {e}\")\n",
    "\n",
    "# %%\n",
    "# Executar o pipeline no notebook\n",
    "extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformar para GeoParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a conversão para GeoParquet...\n",
      "Processando arquivo: adm_embargo_ibama_a.shp...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\adm_embargo_ibama_a.parquet\n",
      "Processando arquivo: Amazonia_2a_atualizacao.shp...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\Amazonia_2a_atualizacao.parquet\n",
      "Processando camada: pa_br_landtenure_imaflora_2021 no arquivo base_fundiaria_imaflora.gpkg...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\base_fundiaria_imaflora_pa_br_landtenure_imaflora_2021.parquet\n",
      "Processando arquivo: biomas_5000.shp...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\biomas_5000.parquet\n",
      "Processando arquivo: BR_Municipios_2023.shp...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\BR_Municipios_2023.parquet\n",
      "Processando arquivo: BR_UF_2023.shp...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\BR_UF_2023.parquet\n",
      "Processando arquivo: Caatinga_2a_atualizacao.shp...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\Caatinga_2a_atualizacao.parquet\n",
      "Processando arquivo: Cerrado_Pantanal_2a_atualizacao.shp...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\Cerrado_Pantanal_2a_atualizacao.parquet\n",
      "Processando arquivo: deter-amz-deter-public.shp...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\deter-amz-deter-public.parquet\n",
      "Processando arquivo: MataAtlantica_2a_atualizacao.shp...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\MataAtlantica_2a_atualizacao.parquet\n",
      "Processando arquivo: Pampa_2a_atualizacao.shp...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\Pampa_2a_atualizacao.parquet\n",
      "Processando camada: pa_br_deforestation_prodes_2002-2022 no arquivo pa_br_deforestation_prodes_2002-2022_IMAFLORA.gpkg...\n",
      "Salvo: C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\pa_br_deforestation_prodes_2002-2022_IMAFLORA_pa_br_deforestation_prodes_2002-2022.parquet\n"
     ]
    }
   ],
   "source": [
    "def transform_to_parquet(source_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process shapefiles and geopackages, converting them to GeoParquet.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando a conversão para GeoParquet...\")\n",
    "    \n",
    "    for root, _, files in os.walk(source_dir):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".shp\") or file_name.endswith(\".gpkg\"):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "\n",
    "                try:\n",
    "                    if file_name.endswith(\".gpkg\"):\n",
    "                        # Obter a lista de camadas corretamente\n",
    "                        layers = fiona.listlayers(file_path)\n",
    "\n",
    "                        for layer in layers:\n",
    "                            print(f\"Processando camada: {layer} no arquivo {file_name}...\")\n",
    "                            gdf = gpd.read_file(file_path, layer=layer)\n",
    "                            output_file = f\"{layer}.parquet\"\n",
    "                            output_path = os.path.join(output_dir, output_file)\n",
    "                            gdf.to_parquet(output_path)\n",
    "                            print(f\"Salvo: {output_path}\")\n",
    "\n",
    "                    else:\n",
    "                        # Processar normalmente os Shapefiles\n",
    "                        print(f\"Processando arquivo: {file_name}...\")\n",
    "                        gdf = gpd.read_file(file_path)\n",
    "                        output_file = os.path.splitext(file_name)[0] + \".parquet\"\n",
    "                        output_path = os.path.join(output_dir, output_file)\n",
    "                        gdf.to_parquet(output_path)\n",
    "                        print(f\"Salvo: {output_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao processar {file_name}: {e}\")\n",
    "                    raise\n",
    "\n",
    "# Definir diretórios e executar no notebook\n",
    "\n",
    "transform_to_parquet(extracted_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validar Geometrias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_geometries(gdf):\n",
    "    \"\"\"\n",
    "    Checks if geometries are valid, attempts to fix invalid geometries,\n",
    "    and drops geometries that remain invalid or are null.\n",
    "    \n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): The input GeoDataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: The cleaned GeoDataFrame with only valid geometries.\n",
    "    \"\"\"    \n",
    "    # Remover geometrias nulas antes de qualquer operação\n",
    "    gdf = gdf[gdf.geometry.notnull()]\n",
    "    \n",
    "    # Verificar geometrias inválidas\n",
    "    invalid_mask = ~gdf.geometry.is_valid\n",
    "    print(f\"Invalid geometries before fix: {invalid_mask.sum()}\")\n",
    "\n",
    "    # Aplicar make_valid() somente em geometrias não nulas\n",
    "    gdf.loc[invalid_mask, \"geometry\"] = gdf.loc[invalid_mask, \"geometry\"].apply(lambda geom: make_valid(geom) if geom else None)\n",
    "\n",
    "    # Verificar novamente após a tentativa de correção\n",
    "    invalid_mask_after = ~gdf.geometry.is_valid\n",
    "    print(f\"Invalid geometries after fix: {invalid_mask_after.sum()}\")\n",
    "\n",
    "    # Remover geometrias que ainda são inválidas ou nulas\n",
    "    gdf = gdf[~invalid_mask_after & gdf.geometry.notnull()]\n",
    "    \n",
    "    print(f\"Final valid geometries count: {len(gdf)}\")\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Áreas embargadas pelo IBAMA\n",
    "gdf_areas_embargadas = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\adm_embargo_ibama_a.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid geometries before fix: 23\n",
      "Invalid geometries after fix: 0\n",
      "Final valid geometries count: 70389\n"
     ]
    }
   ],
   "source": [
    "gdf_areas_embargadas = validate_geometries(gdf_areas_embargadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Junção Espacial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limites administrativos por Estado e Municípios\n",
    "gdf_BR_UF = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\BR_UF_2023.parquet\")\n",
    "gdf_BR_Municipios = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\BR_Municipios_2023.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar a junção espacial para filtrar áreas que intersectam o Brasil\n",
    "gdf_areas_embargadas = gpd.sjoin(gdf_areas_embargadas, gdf_BR_UF, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Remover colunas adicionais da junção, se necessário\n",
    "gdf_areas_embargadas = gdf_areas_embargadas.drop(columns=[\"index_right\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza e Concatenação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áreas prioritárias para conservação\t\n",
    "gdf_APC_amazonia = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\Amazonia_2a_atualizacao.parquet\")\n",
    "gdf_APC_caatinga = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\Caatinga_2a_atualizacao.parquet\")\n",
    "gdf_APC_mata_atlantica = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\MataAtlantica_2a_atualizacao.parquet\")\n",
    "gdf_APC_cerrado_pantanal = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\Cerrado_Pantanal_2a_atualizacao.parquet\")\n",
    "gdf_APC_pampa = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\Pampa_2a_atualizacao.parquet\")\n",
    "\n",
    "datasets = [gdf_APC_pampa, gdf_APC_mata_atlantica, gdf_APC_cerrado_pantanal, gdf_APC_caatinga, gdf_APC_amazonia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_geodataframes(gdf_list):\n",
    "    \"\"\"\n",
    "    Concatenates a list of GeoDataFrames into a single GeoDataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        gdf_list (list): A list of GeoDataFrames to concatenate.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: A concatenated GeoDataFrame.\n",
    "    \"\"\"\n",
    "    if not gdf_list:\n",
    "        raise ValueError(\"The list of GeoDataFrames is empty.\")\n",
    "    \n",
    "    gdf_combined = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True))\n",
    "    return gdf_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_APC_combined = concatenate_geodataframes(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Import_bio\n",
       "Extremamente Alta    625\n",
       "Alta                 487\n",
       "Muito Alta           426\n",
       "Muita Alta             1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_APC_combined[\"Import_bio\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import_bio\n",
      "Extremamente Alta    625\n",
      "Alta                 487\n",
      "Muito Alta           427\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Substituir o valor incorreto \"Muita Alta\" por \"Muito Alto\"\n",
    "gdf_APC_combined[\"Import_bio\"] = gdf_APC_combined[\"Import_bio\"].replace(\"Muita Alta\", \"Muito Alta\")\n",
    "\n",
    "# Verificar se a substituição foi feita\n",
    "print(gdf_APC_combined[\"Import_bio\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronização de geometrias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Verifica o tipo de geometria → Garante que seja um único tipo (Polygon, MultiPolygon, etc.).\n",
    "- Checa coordenadas Z → Se houver Z, pode precisar remover.\n",
    "- Remove geometrias vazias → Evita erros na importação para o PostGIS.\n",
    "- Remove a dimensão Z → Evita problemas se o PostGIS estiver esperando apenas X, Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Polygon' 'MultiPolygon']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Verificar o tipo de geometria\n",
    "print(gdf_APC_combined.geom_type.unique())\n",
    "\n",
    "# Checar se existem dimensões Z na geometria\n",
    "print(gdf_APC_combined['geometry'].has_z.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover dimensões Z, se presentes\n",
    "gdf_APC_combined = gdf_APC_combined[gdf_APC_combined.is_empty==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover a dimensão Z das geometrias\n",
    "func = lambda geom: shapely.wkb.loads(shapely.wkb.dumps(geom, output_dimension=2))\n",
    "gdf_APC_combined['geometry'] = gdf_APC_combined['geometry'].apply(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprojeção do SRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malha fundiária\n",
    "gdf_malha_fundiaria = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\pa_br_landtenure_imaflora_2021.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biomas brasileiros\n",
    "gdf_biomas = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\biomas_5000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desmatamento (PRODES)\n",
    "gdf_desmatamento_prodes = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\pa_br_deforestation_prodes_2002-2022.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desmatamento (DETER)\n",
    "gdf_desmatamento_deter = gpd.read_parquet(r\"C:\\Users\\sandr\\Documents\\GitHub\\spatial-data-management-postgis\\venv\\raw\\output\\deter-amz-deter-public.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_to_sirgas(gdf):\n",
    "    \"\"\"\n",
    "    Reprojects a GeoDataFrame to SIRGAS 2000 Polyconic (EPSG:5880).\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): The input GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: The reprojected GeoDataFrame.\n",
    "    \"\"\"\n",
    "    target_crs = \"EPSG:5880\"  # SIRGAS 2000 Polyconic\n",
    "\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"Input GeoDataFrame does not have a defined CRS.\")\n",
    "\n",
    "    return gdf.to_crs(target_crs) if gdf.crs != target_crs else gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojetar todos os GeoDataFrames para SIRGAS 2000 Polyconic\n",
    "gdf_areas_embargadas = reproject_to_sirgas(gdf_areas_embargadas)\n",
    "gdf_APC_combined = reproject_to_sirgas(gdf_APC_combined)\n",
    "gdf_BR_UF = reproject_to_sirgas(gdf_BR_UF)\n",
    "gdf_BR_Municipios = reproject_to_sirgas(gdf_BR_Municipios)\n",
    "gdf_biomas = reproject_to_sirgas(gdf_biomas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojetar todos os GeoDataFrames para SIRGAS 2000 Polyconic\n",
    "gdf_malha_fundiaria = reproject_to_sirgas(gdf_malha_fundiaria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojetar todos os GeoDataFrames para SIRGAS 2000 Polyconic\n",
    "gdf_desmatamento_prodes = reproject_to_sirgas(gdf_desmatamento_prodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojetar todos os GeoDataFrames para SIRGAS 2000 Polyconic\n",
    "gdf_desmatamento_deter = reproject_to_sirgas(gdf_desmatamento_deter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar variáveis de conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar as variáveis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Recuperar as variáveis de ambiente\n",
    "db_config = {\n",
    "    \"db_user\": os.getenv(\"DB_USER\"),\n",
    "    \"db_password\": os.getenv(\"DB_PASSWORD\"),\n",
    "    \"db_host\": os.getenv(\"DB_HOST\"),\n",
    "    \"db_port\": os.getenv('DB_PORT'),\n",
    "    \"db_name\": os.getenv(\"DB_NAME\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexão com o Banco de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_connection(db_user, db_password, db_host, db_port, db_name):\n",
    "    \"\"\"Cria uma conexão com o banco de dados PostgreSQL.\"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",  # Conexão inicial ao banco padrão\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        host=db_host,\n",
    "        port=db_port\n",
    "    )\n",
    "    conn.autocommit = True\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_database(cursor, db_name):\n",
    "    \"\"\"Inicializa o banco de dados, criando-o se necessário.\"\"\"\n",
    "    try:\n",
    "        cursor.execute(f\"CREATE DATABASE {db_name};\")\n",
    "        print(f\"Banco de dados '{db_name}' criado com sucesso!\")\n",
    "    except psycopg2.errors.DuplicateDatabase:\n",
    "        print(f\"O banco de dados '{db_name}' já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O banco de dados 'spatial_data_warehouse' já existe.\n"
     ]
    }
   ],
   "source": [
    "# Inicialização\n",
    "conn = create_database_connection(**db_config)\n",
    "cur = conn.cursor()\n",
    "initialize_database(cur, db_config[\"db_name\"])\n",
    "conn.close()\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{db_config['db_user']}:{db_config['db_password']}@{db_config['db_host']}:{db_config['db_port']}/{db_config['db_name']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar Esquemas e extensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_schemas(engine, schemas):\n",
    "    \"\"\"Cria os esquemas necessários no banco de dados.\"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        for schema in schemas:\n",
    "            try:\n",
    "                conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {schema};\"))\n",
    "                print(f\"Esquema '{schema}' criado com sucesso!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criar o esquema '{schema}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extensions(engine, extensions):\n",
    "    \"\"\"Habilita as extensões necessárias no banco de dados.\"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        for ext in extensions:\n",
    "            try:\n",
    "                conn.execute(text(ext))\n",
    "                print(f\"Extensão executada com sucesso: {ext}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criar extensão: {ext} -> {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extensão executada com sucesso: CREATE EXTENSION IF NOT EXISTS postgis;\n",
      "Extensão executada com sucesso: CREATE EXTENSION IF NOT EXISTS postgis_raster;\n",
      "Extensão executada com sucesso: CREATE EXTENSION IF NOT EXISTS h3_postgis CASCADE;\n",
      "Esquema 'ibge' criado com sucesso!\n",
      "Esquema 'imaflora' criado com sucesso!\n",
      "Esquema 'ibama' criado com sucesso!\n",
      "Esquema 'inpe' criado com sucesso!\n",
      "Esquema 'icmbio' criado com sucesso!\n",
      "Esquema 'sicar' criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Extensões e esquemas\n",
    "extensions = [\n",
    "    \"CREATE EXTENSION IF NOT EXISTS postgis;\",\n",
    "    \"CREATE EXTENSION IF NOT EXISTS postgis_raster;\",\n",
    "    \"CREATE EXTENSION IF NOT EXISTS h3_postgis CASCADE;\"\n",
    "]\n",
    "schemas = [\"ibge\", \"imaflora\", \"ibama\", \"inpe\", \"icmbio\",\"sicar\"]\n",
    "\n",
    "#%%\n",
    "create_extensions(engine, extensions)\n",
    "create_schemas(engine, schemas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar para o PostGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_to_postgis(gdf, table_name, schema, engine):\n",
    "    \"\"\"\n",
    "    Exporta o GeoDataFrame para o banco de dados PostgreSQL.\n",
    "    \"\"\"\n",
    "    gdf.to_postgis(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        schema=schema,\n",
    "        if_exists=\"replace\",  # ou \"append\" dependendo da lógica\n",
    "        index=False,\n",
    "    )\n",
    "    print(f\"Dados exportados com sucesso para a tabela {schema}.{table_name}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_partitioned_to_postgis(gdf, engine, schema, column, prefix):\n",
    "    \"\"\"\n",
    "    Partitions a GeoDataFrame by a given column (e.g., state abbreviation) and exports each partition to PostGIS.\n",
    "    \n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): The input GeoDataFrame containing the spatial data.\n",
    "        engine (SQLAlchemy engine): The connection engine to the PostGIS database.\n",
    "        schema (str): The target schema in the database.\n",
    "        column (str): The column used for partitioning (for example: \"sigla_uf\").\n",
    "        prefix (str): The prefix for table names (for example: \"base_fundiaria\").\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Obter valores únicos da coluna de partição\n",
    "    unique_values = gdf[column].unique()\n",
    "\n",
    "    # Percorrer cada valor único e exportar separadamente\n",
    "    for value in unique_values:\n",
    "        # Filtrar os dados pelo valor atual\n",
    "        gdf_partition = gdf[gdf[column] == value]\n",
    "\n",
    "        # Nome da tabela no banco\n",
    "        table_name = f\"{prefix}_{value}\"\n",
    "\n",
    "        # Enviar para o banco de dados\n",
    "        gdf_partition.to_postgis(name=table_name, con=engine, schema=schema, if_exists='replace', index=False)\n",
    "        print(f\"Tabela '{table_name}' enviada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados exportados com sucesso para a tabela ibama.areas_embargadas!\n",
      "Dados exportados com sucesso para a tabela ibama.areas_prioritarias_conservacao!\n",
      "Dados exportados com sucesso para a tabela ibge.estados_br!\n",
      "Dados exportados com sucesso para a tabela ibge.municipios_br!\n",
      "Dados exportados com sucesso para a tabela icmbio.biomas!\n"
     ]
    }
   ],
   "source": [
    "# Exportar para o PostgreSQL/PostGIS\n",
    "export_to_postgis(gdf_areas_embargadas, \"areas_embargadas\", \"ibama\", engine)\n",
    "export_to_postgis(gdf_APC_combined, \"areas_prioritarias_conservacao\", \"ibama\", engine)\n",
    "export_to_postgis(gdf_BR_UF, \"estados_br\", \"ibge\", engine)\n",
    "export_to_postgis(gdf_BR_Municipios, \"municipios_br\", \"ibge\", engine)\n",
    "export_to_postgis(gdf_biomas, \"biomas\", \"icmbio\", engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela 'desmatamento_prodes_caatinga' enviada com sucesso!\n",
      "Tabela 'desmatamento_prodes_cerrado' enviada com sucesso!\n",
      "Tabela 'desmatamento_prodes_matlantica' enviada com sucesso!\n",
      "Tabela 'desmatamento_prodes_pampa' enviada com sucesso!\n",
      "Tabela 'desmatamento_prodes_pantanal' enviada com sucesso!\n",
      "Tabela 'desmatamento_prodes_amz' enviada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Exportar a camada de destamatamento PRODES particionado por Bioma\n",
    "export_partitioned_to_postgis(gdf_desmatamento_prodes,engine,'inpe','source','desmatamento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela 'desmatamento_deter_PA' enviada com sucesso!\n",
      "Tabela 'desmatamento_deter_AM' enviada com sucesso!\n",
      "Tabela 'desmatamento_deter_AC' enviada com sucesso!\n",
      "Tabela 'desmatamento_deter_MT' enviada com sucesso!\n",
      "Tabela 'desmatamento_deter_RO' enviada com sucesso!\n",
      "Tabela 'desmatamento_deter_MA' enviada com sucesso!\n",
      "Tabela 'desmatamento_deter_TO' enviada com sucesso!\n",
      "Tabela 'desmatamento_deter_RR' enviada com sucesso!\n",
      "Tabela 'desmatamento_deter_AP' enviada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Exportar a camada de destamatamento DETER particionado por Estado\n",
    "export_partitioned_to_postgis(gdf_desmatamento_deter,engine,'inpe','UF','desmatamento_deter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela 'malha_fundiaria_AP' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_RR' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_PA' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_AM' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_MT' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_RO' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_CE' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_MA' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_PI' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_ES' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_RN' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_PE' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_TO' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_PB' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_AC' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_GO' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_DF' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_BA' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_PR' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_SC' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_RS' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_AL' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_SE' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_MG' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_RJ' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_SP' enviada com sucesso!\n",
      "Tabela 'malha_fundiaria_MS' enviada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Exportar a malha fundiária particionada por Estado\n",
    "export_partitioned_to_postgis(gdf_malha_fundiaria,engine,'imaflora','sigla_uf','malha_fundiaria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fechar as conexões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fechar a conexão administrativa com o PostgreSQL\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
